\section{Introduction}
This paper is about an academic project born to be an efficient solution for the CINI\footnote{Consorzio Interuniversitario Nazionale per l'Informatica} 2017 Challenge on smart cities illumination systems. In particular, the goal was to prototype and test a solution which was capable of (near) real-time data stream processing for monitoring records from street lamps, lumen sensors co-located with the street lamp itself and from traffic data produced by third-party APIs. We will explore this solution for the following use case: in a smart city context it is necessary to guarantee the maximum efficiency from lamps consumption while providing an optimal illumination within safety limits for pedestrians and drivers and according to local traffic intensity. To achieve that, it is necessary to project a grid of smart lamps capable of tuning their light level according to the right amount of energy necessary to provide city aware, safe and green consumption levels. This grid must be powered and managed via a reliable, highly available, processing-capable control system. Introducing Project Ember.

\section{Frameworks and Tools}
We structured our environment using at first a publish/subscribe architecture with street lamps, lumen and traffic sensors as publishers and the stream processing framework as subscriber. Talking about Publish/Subscribe frameworks, different offers are available over the open source community. RabbitMQ\footnote{RabbitMQ documentation: https://www.rabbitmq.com/getstarted.html} and Apache Kafka\footnote{Apache Kafka documentation: https://kafka.apache.org/documentation/} were our favorite choices, but the exploration of the documentation of that which would be our Data Stream Processing framework made possible the final decision.
Java is the programming language that links all these components together being used by Flink and APIs for the components we chose.  

\subsection{Data stream processing}
The Apache Software Foundation makes available different alternatives, each one a refined version of the previous: Apache Storm\footnote{Apache Storm official page: http://storm.apache.org/} is one of the most used data stream processing framework on the market and one of the most supported as well; Apache Spark\footnote{Apache Spark official page: http://spark.apache.org/} is a refined version of Storm even though it is limited to fewer programming languages (Java, Scala and Python); Apache Flink\footnote{Apache Flink official page: https://flink.apache.org/} is the most recent project among the three and the most advanced. Flink gives the programmer the possibility to define just the topology of the operators, how they are linked, or how to set the windows timing (based upon the event time or upon the processing time spent inside the system). Flink thinks about the rest: multithreading, synchronization, parallelism, availability, cluster management are our interest no more, it takes care of them for us. We chose the latest stable release of Apache Flink, 1.2, which comes with a well written documentation as well as multiple connectors for the most popular MOM or persistency platform. Flink calls a Source that component that produce data and Sink the one that takes the processed data in order to persistence or to route them. 

\subsection{Connectors}
Apache Kafka works seamlessly with Flink thanks to the included connector plugins giving the possibility to simply personalize the connection according to our preferences: Kafka is our Source. Talking about Sink, Flink supports many platforms and Kafka can be one of them, but in order to persist and manage our data we wanted to use a modern platform capable of organizing data for (near) real-time purposes, properties not so common for databases with RDBMS engines. A NoSQL approach was mandatory to us, so we analyzed different products: Elasticsearch\footnote{Elasticsearch official page: https://www.elastic.co/products/elasticsearch} and Apache Cassandra\footnote{Apache Cassandra official page: http://cassandra.apache.org/}. Elasticsearch is fully supported (not its last version) by Flink 1.2 and is a flexible, easy-to-deploy database with RESTful APIs. It makes possible every type of query, even geographical ones, in a fast and simple way. Elasticsearch is part of the Elastic Stack built by Elastic.co which makes available another useful tool for visualizing data stored in Elasticsearch linking and visualizing them according to our preferences, Kibana\footnote{Kibana official page: https://www.elastic.co/products/kibana}. That's why Apache Cassandra was not chosen at last although it seemed obvious for us at first, being part of the Apache Software Foundation and respecting the configuration formats of the other components of the system.

\subsection{Persistence level}
The choice of the framework that would realize our persistence level has been influenced by the search of a simple and elegant visualization platform for our data. Elasticsearch has Kibana, but we really wanted to use Grafana\footnote{Grafana official page: https://grafana.com/} at first. Grafana offers different connector for the most famous persistence platform, Elasticsearch included. Unfortunately its deploying and configuration looked complicated to configure in order to obtain even an adequate visualization and time wasn't on our side. Kibana instead was incredibly easy to deploy, having only to specify the Elasticsearch database you want to use, understanding the data format inside the database. 

\subsection{Extras}
What we called control unit is a component that will analyze later in this paper. It has been developed using Flask and Redis. Flask\footnote{Flask official page: http://flask.pocoo.org/} is a micro-framework for Web Server built in Python and Redis\footnote{Redis official page: https://redis.io/} is one of the most famous in-memory data store. We used Flask a lot for realizing web servers. It allows us to define entrypoint with a single line of code and RESTful APIs in seconds. Flask is also very efficient being multithreaded and easy to deploy thanks to Python. Redis instead is a really fast data store and works as a database, cache and message broker. Redis serves us as a cache, maintaining a good amount of data per control unit but allowing us to interact with it with very simple APIs and to deploy it easily as well.

\section{Architecture overview}
In this section we will cover how the system communicates between each of its components and modules and the assumptions we made to prototype and test the architecture. In \hyperref[fig:ember_architecture]{figure 1} a high-level architecture overview is provided. Before proceeding, we want to focus on the output from the real-time\footnote{We will define the system as "real-time" in this paper even if is not validated for such a control system, but it is capable of near real-time data streams processing} control system: it is produced into the MOM\footnote{Messages Oriented Middleware} and consumed by control units (how will be discussed later), closing a feedback loop. This behavior and the capabilities to maintain high-availability across the clusters make the system itself near to the features of an autonomic system.
\begin{figure}
\begin{center}
	\includegraphics[scale=0.35]{img/ember_architecture}
	\caption{Project Ember architecture overview}
	\label{fig:ember_architecture}
\end{center}
	
\end{figure}


\subsection{Sensors network}
First of all let us consider how the sensors network [...]. According to project specifications the street lamps sends to the system a JSON formatted string containing all the information their micro-controllers collect. Those data are sent every 10 seconds. A lumen sensor is placed on the lamp and it sent data with same rate as well giving us information about the daylight luminosity level and the id of the lamps he is placed upon.

\subsection{KAFKA TO FLINK}
Kafka manages all the messages that comes from lamps (e.g. control unit), lumen and traffic sensors. Being more specific, the lumen sensor sends to Kafka a JSON formatted string containing the ID of the lamp where it is placed upon (or another value if it is a sensor for a whole street or district), the street where it is located, and the luminosity value record a the timestamp also included. The traffic sensor instead does not need an ID, but the street and the timestamp of the moment when the data is retrieved, with the value of the traffic recorded. Those data are sent every 10 seconds in our scenario. Kafka puts these messages in other two different topics: lumen and traffic. Flink is registered on the main three topics and processes them calculating the right lumen value a lamp should have, a rank of the most-probable-to-fail lamps and consumption means per streets and/or per ids. Some data needs to be sent the right lamp in order to obtain the adaptation we need to perform, while the others need to be seen by supervisors in a well designed and easy to explore dashboard, which is Elasticsearch plus Kibana. That’s where Flink comes, consuming these messages in order to calculate and provide the adaptation and the data we need. Flink registers itself as a Consumer to Kafka using its plugin. The KafkaFlinkConsumer class allows to define the topic where to take messages, the options of the kafka brokers where we are linking to and a Serialization Schema which can be a predefined class or a custom one implementing a common interface. We decided to use a SimpleStringSchema to retrieve our data, converting the messages from bytes to a string representation. The string is mapped to the proper class using the GSON library which handles all the conversion tasks for us. Once obtained our object we have to assign it a timestamp. Such necessity is mandatory for ordering all the data using their so called Event Time. The Event Time is the instant of time when the data is created by its source. This time comes useful for us for creating windows of time where to aggregate messages.


\subsection{FLINK}
Where the stream processing happens. After data conversion and timestamp assignment, the object passes through different operators; the topology is presented here.


Lamp objects are aggregated according to a single common key per group, that is the lamp id. Using the same concept we aggregate lumens by address and traffics by address.
We now have to aggregate by windows. Being the data sent every 10 seconds for all sensors, we decide to use a windows of 10 seconds long. A window creates an iterable object containing all the objects that fell in the chosen (and so, customizable) time span respecting the same key (id or address depending on the object). In order to provide street lamp adaptation we have to determine the luminosity and traffic level on a street so we have to calculate a mean for the luminosity and traffic values for each street. The Window allows us to do so according to a common key and a reasonable time span. Means given, we can now join the two streams of means grouping them by the common address attribute and aggregating them by the last 30 seconds of data. That’s where our concept of time change. Because data are first aggregated by their generation time, we now have to aggregate them by their processing time, which is the time spent inside the system itself. So the window operator will gather all the means from the first to the last one that arrives at most after 30 seconds from the first one. Having a new window we can now create a new object which contains a key, the street that the means shared, and the luminosity and traffic means values.
Such aggregatedSensorStream has to be joined with the streetLampBuffer containing lamps aggregated by id in the last 10 seconds. The new stream is obtained by grouping them by the address and windowing them in a 60 seconds time span according to the processing time they spent. Once the window is created we can calculate the optimal value for each lamp
(DESCRIVERE CALCOLO)
The controlStream obtained is directed to Kafka using a wrapper for the KafkaFlinkProducer class (DA PARLARE DOPO ?)
The streetLampBuffer also gives us the informations we need for creating a rank of the most probable-to-fail lamps that is, ordering them basing upon their replacement time span. A sliding Windows is used with a personalizable timestamp as well as the top list (3, 5, 10 or as much as the user wants). Other two windows are created using the lamp buffer keyed by id and by address. These two windows aggregates data of the last hour and calculate the consumption mean of the single lamp in the last time span or of a particular street on the same time span.

\subsection{FLINK TO ELASTICSEARCH/KIBANA}
The ranks, the consumption mean by id and by address, have to be visualized and showed in real-time, so Elasticsearch comes. Flink offers an interface for Elasticsearch, that, simply providing the cluster configuration, connects to it and allows to send data in byte encoded json format strings. Elasticsearch organizes data by so called Indexes and Types. The Index is the equivalent of Database in a NoSQL approach and the Types the equivalent of Tables. Elasticsearch keeps tracks of data assigning them a mapping table so it can be capable of understand primitives or complex data types. Kibana comes in our help for representing them. By a simple file of configuration you can use Kibana to create your own dashboard by defining all the data interpreter specifying the Type it has to use, how rearrange the attributes, how ordering and in which style visualize them. Elasticsearch and Kibana has been designed so they easily interact one another and simply deployed as well. Elasticsearch can be extended without any amount of effort, simply defining new nodes for the cluster and joining the master node. Elasticsearch serves also a particular role: being designed as a RESTful engine as well, it allows to specify complex query and resolves them in a split second. We use Elasticsearch as the native source of alerts, by simply find all those lamps that has not been updated for more than a customizable amount of time. Infact all the data that lamps send to use are stored in Elasticsearch so we can have a complete control over the lamps and how they are communicating with us, respecting their timing and their orders.

\section{A Message oriented approach}
Apache Kafka plays its MOM role connecting all the components of our system so let’s see how they are related to each other thanks to it: Lamps and Flink produce a lot of data in streams that are organized in topics: lamps, lumens and traffic, being specific. A system such as this can be sufficient but let’s think: being a Smart Lamp Grid the system has to emit orders directed to a single lamp among thousands. Kafka can manage runtime topics creation but a topic for a single lamp and each lamp registered to its topic is meaningless and inefficient; requiring the data stream processing system to know the right position of the lamp among network is infeasible as well.

\subsection{Sensors to Kafka}
Lamps are supposed to be given, with a micro-controller built inside them capable to connect to the city intranet, to understand the power level proper of the bulb model and how such parameters relates one another in order to obtain the right luminosity. A lamp produce a JSON formatted string containing: the lamp id, the model of the light bulb, the timestamp of the last replacement instant of time, the current power consumption, the luminosity level, the control unit where it is registered, etc. 
INSERIRE JSON

The lumen sensor is placed upon the street lamp and is managed by the same micro-controller; lumen sensors common for entire streets are managed as well. The lumen produce a JSON formatted string containing the same id of the lamp where it is placed upon (or a nonce for the common sensors), the luminosity value recorded, the street where it is located and the timestamp of the time instant when the data has been recorded. 

The traffic sensor is realized by third party APIs and it is registered to Kafka producing a JSON formatted string containing the street monitored, the traffic value and the timestamp of the recorded time.

\subsection{Kafka to Adapters}
Once Flink calculates the state a street lamp should have it sends control directives to custom topics. Each lamps, in fact, is linked to a control unit and this information is included in the JSON sent by lamps to the system. That’s where the control directive is routed, a topic that is identified by the string that identify the control unit responsible of a particular street lamp; so we can, on one hand, free Kafka of an incredible number of topics and, on the other hand, free smart lamps to register themselves on their ones. As we said even the alerts of lamps not functioning or not communicating for a too long time can be routed to Kafka. Such alarms are simply transmitted to it using the connector provided by Flink that requires a byte encoding of the Alert object and the specification of the topic where they are directed to. 

\subsection{Control Unit}
We needed a web server component capable of directing lamps data to Kafka and directive from Flink to lamps. The control unit has been built with such purpose in mind giving us the possibility to create a new indirection level that is placed among lamps and Kafka/Flink. Lamps talks to the control unit which maintains a mapping between the lamps IDs and their IP addresses inside the local city network. Lamps have to be registered to the right control unit in order to submit their data inside the system which increases the security level of our system (once placed, the lamp has to be registered by the local control unit, which could manage a street or a district. Such operation needs to be done by an operator that places the lamp). The control unit validates the lamps and sends their data to the Kafka lamp topic. The control unit takes care to register itself to the topic of its own, so it can read the responses and understands to which lamp direct them. Once the response will be made available the control unit reads the messages and convert them to a JSON object so it can access their attributes; it determines the id of the lamp and check in the Redis database to find the ip address related to that lamp id. That’s the core of the indirection level of the whole architecture. Kafka and the control unit makes possible all of this, making our system totally plug and play. 

% to insert a simple bar chart
%\begin{tikzpicture}
%\begin{axis}[
%	x tick label style={
%		/pgf/number format/1000 sep=},
%	ylabel=Year,
%	enlargelimits=0.05,
%	legend style={at={(0.5,-0.1)},
%	anchor=north,legend columns=-1},
%	ybar interval=0.7,
%]
%\addplot 
%	coordinates {(2012,408184) (2011,408348)
%		 (2010,414870) (2009,412156) (2008,415 838)};
%\addplot 
%	coordinates {(2012,388950) (2011,393007) 
%		(2010,398449) (2009,395972) (2008,398866)};
%\legend{Men,Women}
%\end{axis}
%\end{tikzpicture}